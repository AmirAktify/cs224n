\documentclass{article}

\title{CS224N: Gradient Notes}
\author{Amirali Abdullah}
\date{Winter 2020}
\usepackage{graphicx}
\usepackage{amsmath} 
\usepackage{multicol}
\newenvironment{docspec}{\begin{quote}\noindent}{\end{quote}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\textunderscript}[1]{$_{\text{#1}}$}
\allowdisplaybreaks
\setcounter{secnumdepth}{3}
\begin{document}

\section{Gradient formulas}

\subsection{Cross entropy and softmax}
Consider vectors $p$ and $q$ and let $p = softmax(\theta)$.
Further define the cross entropy of $J = CE(p,q) = \sum_i p_i \log q_i$


\end{document}